{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6815698b-63b1-4023-8ac5-6245dcb6a4d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# BRONZE LAYER\n",
    "\n",
    "## General Info\n",
    "  | Info | Details |\n",
    "  |---------|------|\n",
    "  |Table Name | customers + sales|\n",
    "  |From | transient (Azure) |\n",
    "\n",
    "## Update timeline\n",
    "|Date | Developed/altered by: | Comment |\n",
    "|:------:|--------|-------|\n",
    "|2025/01/23|Luca Ainstein|Project Data Ingestion|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0302d742-8af9-458a-a57c-f25614b31751",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f22dcaf-4861-42e5-9f14-edfd44cc0f9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### Import of libs\n",
    "\n",
    "from pyspark.sql.functions import current_date, current_timestamp, expr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "138ee671-1ea7-4fda-bd00-16702de864a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Cluster Conection to Azure Transient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "461509aa-d15f-4621-9849-ea9a6da33573",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration applied successfully.\n"
     ]
    }
   ],
   "source": [
    "### There is the alternative of using the conection directly on the cluster creation. However, since the Databricks Community Cloud only allows 1-2 hours of inactivity before deleting the cluster, it is better to create the connection on the code. \n",
    "\n",
    "# Set up the configurations for Azure Data Lake Storage Gen2 using OAuth authentication\n",
    "configs = {\n",
    "    \"fs.azure.account.auth.type.dlsprojetofixo.dfs.core.windows.net\": \"OAuth\",\n",
    "    \"fs.azure.account.oauth.provider.type.dlsprojetofixo.dfs.core.windows.net\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "    \"fs.azure.account.oauth2.client.id.dlsprojetofixo.dfs.core.windows.net\": \"238b3ff5-2a4b-49a0-838e-61aea5e55f0a\",\n",
    "    \"fs.azure.account.oauth2.client.secret.dlsprojetofixo.dfs.core.windows.net\": \"~h.8Q~XwXAMBiXDY69nUxONcfoO49RweWIPELdkM\",\n",
    "    \"fs.azure.account.oauth2.client.endpoint.dlsprojetofixo.dfs.core.windows.net\": \"https://login.microsoftonline.com/d16f0536-3c2f-4035-887f-8949bacfacfd/oauth2/token\"\n",
    "}\n",
    "\n",
    "# Apply the configurations to Spark\n",
    "for key, value in configs.items():\n",
    "    spark.conf.set(key, value)\n",
    "\n",
    "# Verify the configuration\n",
    "print(\"Configuration applied successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c72c0304-decd-48a4-b2f7-122ab3fdb5a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Import Dataset 1 (**customers**) `.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a54f14b7-5fa1-4036-93d1-ff9adb7d5324",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "path_source = 'abfss://transient@dlsprojetofixo.dfs.core.windows.net/ecommerce_customers_500.csv'\n",
    "\n",
    "df = spark.read.format(\"csv\").option(\"header\", True).load(path_source).withColumn(\"load_time_GMT\", current_timestamp())\n",
    "\n",
    "## Comment: the time stamp is loaded in GMT (consider that Brasil is GMT -3:00)\n",
    "\n",
    "# On a real life import, check the best practice on the company: if they prefer local time or a standar time (as IST, EST, etc)\n",
    "\n",
    "# Note for Silver developer: note that AGE is a STRING. Must be turn into a INT on silver layer (of even float, depending on the data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c487ef74-05c5-4712-b48c-44d0e78500ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Schema Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "541e4e98-33aa-44b4-9b60-ed23428dbd2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema is created!\n"
     ]
    }
   ],
   "source": [
    "schema_layer = \"bronze\"\n",
    "table_name = \"customers\"\n",
    "\n",
    "path_target = 'abfss://bronze@dlsprojetofixo.dfs.core.windows.net/clientes_lucaainstein'\n",
    "\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS bronze\")\n",
    "df.write \\\n",
    "    .format('delta') \\\n",
    "    .mode('overwrite') \\\n",
    "    .option('path', path_target) \\\n",
    "    .option('overwriteSchema', 'true') \\\n",
    "    .saveAsTable(f'{schema_layer}.{table_name}')\n",
    "print(\"Schema is created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10c74709-92f2-42ee-9b96-5297b24fc6ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization bronze.customers completed!\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"OPTIMIZE {schema_layer}.{table_name}\")\n",
    "print(\"Optimization \" + f'{schema_layer}.{table_name}' + \" completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58f91350-341c-4d0a-ba2a-65a2bee8d90b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Import Dataset 2 (**sales**) `.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57f0ab8a-cbe9-4f85-b724-da00d7be4048",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "path_source = 'abfss://transient@dlsprojetofixo.dfs.core.windows.net/ecommerce_large.json'\n",
    "\n",
    "df = spark.read.format(\"json\").option(\"multiline\", True).load(path_source).withColumn(\"load_time_GMT\", current_timestamp())\n",
    "\n",
    "## Comment: the time stamp is loaded in GMT (consider that Brasil is GMT -3:00)\n",
    "\n",
    "# On a real life import, check the best practice on the company: if they prefer local time or a standar time (as IST, EST, etc)\n",
    "\n",
    "# Note for Silver developer: note that AGE is a STRING. Must be turn into a INT on silver layer (of even float, depending on the data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cdaeb4f-9250-43dd-8965-595c4d3ceff4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema is created!\n"
     ]
    }
   ],
   "source": [
    "schema_layer = \"bronze\"\n",
    "table_name = \"sales\"\n",
    "\n",
    "path_target = 'abfss://bronze@dlsprojetofixo.dfs.core.windows.net/sales_lucaainstein'\n",
    "\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS bronze\")\n",
    "df.write \\\n",
    "    .format('delta') \\\n",
    "    .mode('overwrite') \\\n",
    "    .option('path', path_target) \\\n",
    "    .option('overwriteSchema', 'true') \\\n",
    "    .saveAsTable(f'{schema_layer}.{table_name}')\n",
    "print(\"Schema is created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c0a7105-f50f-446b-8dfa-401bc2e72c2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization bronze.sales completed!\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"OPTIMIZE {schema_layer}.{table_name}\")\n",
    "print(\"Optimization \" + f'{schema_layer}.{table_name}' + \" completed!\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 238860022581199,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_layer_config",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
